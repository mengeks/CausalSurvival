# Stability Analysis of Cox Proportional Hazards Model


## Introduction

The Cox proportional hazards model is widely used in survival analysis, but its stability can be compromised when dealing with extreme values in the hazard function. This analysis investigates how the model's performance degrades when the linear predictor (β'X) becomes large, particularly comparing a standard implementation (coxph) with a custom likelihood optimizer designed to handle extreme values.

Of particular interest is the stability of coefficient estimation when:
1. The true coefficients are scaled to larger magnitudes
2. Additional offset terms are introduced in the hazard function

## Setup

```{r setup, message=FALSE}
library(here)
library(survival)
library(ggplot2)
library(tidyr)
library(dplyr)
library(knitr)

# Source helper functions
source(here("R/cox-loglik.R"))
source(here("tests/test-helper-cox-loglik.R"))

# Set random seeds for reproducibility
seed_values <- c(123, 321, 231)
```

## Data Generation Process

We simulate survival data under two scenarios to systematically evaluate model stability:

1. **Without Offset**:
   - Covariates: $X_{i,j} \sim \mathcal{N}(0, 1)$
   - Survival times: $T_i \sim \text{Exp}(h_0 \cdot \exp(X_i \beta))$
   - Censoring: $\delta_i \sim \text{Bernoulli}(p_{\text{event}})$

2. **With Offset**:
   - Additional offset term: $O_i \sim \mathcal{N}(2, 1)$
   - Survival times: $T_i \sim \text{Exp}(h_0 \cdot \exp(X_i \beta + O_i))$

Where:
- $h_0 = 0.1$ (baseline hazard)
- $p_{\text{event}} = 0.7$ (event probability)
- $n = 1000$ observations
- $p = 3$ covariates
- True coefficients: β = (0.5, -0.3, 0.2) × multiplier

We examine two coefficient magnitudes by setting multiplier = 1 or 8.





## Analysis Function

```{r}
run_analysis <- function(seed_value, beta_multiplier = 1) {
  set.seed(seed_value)
  n <- 1000
  p <- 3
  true_beta <- c(0.5, -0.3, 0.2) * beta_multiplier
  init_beta <- rep(0, p)
  
  # Without offset
  data_wo <- generate_cox_data(n = n, p = p, true_beta = true_beta, offset = 0)
  result_wo <- fit_custom_cox_model(init_beta, data_wo$time, data_wo$status, 
                                  data_wo$covar, data_wo$strata, data_wo$offset)
  
  # Fit coxph without offset
  df_fit_wo <- data.frame(
    time = data_wo$time,
    status = data_wo$status,
    covar.1 = data_wo$covar[,1],
    covar.2 = data_wo$covar[,2],
    covar.3 = data_wo$covar[,3],
    offset = data_wo$offset
  )
  formula_wo <- as.formula("Surv(time, status) ~ covar.1 + covar.2 + covar.3 + offset(offset)")
  coxph_fit_wo <- coxph(formula_wo, data = df_fit_wo, control = coxph.control())
  cox_beta_wo <- coef(coxph_fit_wo)
  loglik_wo_cox_beta <- cox_loglik(
    beta = cox_beta_wo,
    time = data_wo$time,
    status = data_wo$status,
    covar = data_wo$covar,
    strata = data_wo$strata,
    offset = data_wo$offset
  )
  
  # With offset
  offset_vector <- rnorm(n, mean = 2, sd = 1)
  data_w <- generate_cox_data(n = n, p = p, true_beta = true_beta, offset = offset_vector)
  result_w <- fit_custom_cox_model(init_beta, data_w$time, data_w$status, 
                                 data_w$covar, data_w$strata, data_w$offset)
  
  # Fit coxph with offset
  df_fit_w <- data.frame(
    time = data_w$time,
    status = data_w$status,
    covar.1 = data_w$covar[,1],
    covar.2 = data_w$covar[,2],
    covar.3 = data_w$covar[,3],
    offset = data_w$offset
  )
  formula_w <- as.formula("Surv(time, status) ~ covar.1 + covar.2 + covar.3 + offset(offset)")
  coxph_fit_w <- coxph(formula_w, data = df_fit_w, control = coxph.control())
  cox_beta_w <- coef(coxph_fit_w)
  loglik_w_cox_beta <- cox_loglik(
    beta = cox_beta_w,
    time = data_w$time,
    status = data_w$status,
    covar = data_w$covar,
    strata = data_w$strata,
    offset = data_w$offset
  )
  
  list(
    true_beta = true_beta,
    wo_offset = result_wo$par,
    w_offset = result_w$par,
    cox_wo_offset = cox_beta_wo,
    cox_w_offset = cox_beta_w,
    loglik_wo = result_wo$value,
    loglik_w = result_w$value,
    loglik_cox_wo = loglik_wo_cox_beta,
    loglik_cox_w = loglik_w_cox_beta
  )
}
```

## Results

```{r message=FALSE, warning=FALSE, results='hide'}
# Run analysis for different seeds and multipliers
results <- list()
for (mult in c(1, 8)) {
  for (seed in seed_values) {
    results[[paste(seed, mult)]] <- run_analysis(seed, mult)
  }
}
```

```{r}
source(here("scripts/TV-CSL/tests/test-helper.R"))

create_comparison_table <- function(results) {
  rows <- list()
  
  for (name in names(results)) {
    r <- results[[name]]
    seed_mult <- strsplit(name, " ")[[1]]
    
    # Calculate bias and RMSE for both custom and coxph
    bias_wo <- (r$wo_offset - r$true_beta) / r$true_beta
    bias_w <- (r$w_offset - r$true_beta) / r$true_beta
    bias_cox_wo <- (r$cox_wo_offset - r$true_beta) / r$true_beta
    bias_cox_w <- (r$cox_w_offset - r$true_beta) / r$true_beta
    
    rmse_wo <- sqrt(mean((r$wo_offset - r$true_beta)^2))
    rmse_w <- sqrt(mean((r$w_offset - r$true_beta)^2))
    rmse_cox_wo <- sqrt(mean((r$cox_wo_offset - r$true_beta)^2))
    rmse_cox_w <- sqrt(mean((r$cox_w_offset - r$true_beta)^2))
    
    # Calculate min and max relative errors
    rel_errors_wo <- calculate_relative_errors(r$true_beta, r$wo_offset)
    rel_errors_w <- calculate_relative_errors(r$true_beta, r$w_offset)
    rel_errors_cox_wo <- calculate_relative_errors(r$true_beta, r$cox_wo_offset)
    rel_errors_cox_w <- calculate_relative_errors(r$true_beta, r$cox_w_offset)
    
    
    rows[[name]] <- data.frame(
      Seed = seed_mult[1],
      Multiplier = seed_mult[2],
      RMSE_custom_wo = rmse_wo,
      RMSE_custom_w = rmse_w,
      RMSE_coxph_wo = rmse_cox_wo,
      RMSE_coxph_w = rmse_cox_w,
      max_rel_err_custom_wo = rel_errors_wo$max$max_relative_error,
      max_rel_err_custom_w = rel_errors_w$max$max_relative_error,
      max_rel_err_coxph_wo = rel_errors_cox_wo$max$max_relative_error,
      max_rel_err_coxph_w = rel_errors_cox_w$max$max_relative_error,
      LogLik_custom_wo = r$loglik_wo,
      LogLik_custom_w = r$loglik_w,
      LogLik_coxph_wo = r$loglik_cox_wo,
      LogLik_coxph_w = r$loglik_cox_w
    )
  }
  
  do.call(rbind, rows)
}
comparison_table_full <- create_comparison_table(results)

comparison_table <- comparison_table_full %>%
  select(Seed, Multiplier, ends_with("_w"))
library(kableExtra)
kable(comparison_table, 
      digits = 4,
      caption = "Comparison of Models with Offset",
      format = "markdown",
      align = c('l', 'l', rep('r', 6))) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```
## Results and Discussion

### Impact of Coefficient Magnitude

When analyzing the estimation accuracy with multiplier = 1, the true coefficient vector is (0.5, -0.3, 0.2). Both the custom likelihood optimizer and coxph perform similarly well:
```r
# Example from one seed
summary(data_small$time)  # Add actual summary statistics
```

However, with multiplier = 8, the true coefficient vector becomes (4.0, -2.4, 1.6), leading to much larger survival times:
```r
summary(data_large$time)  # Add actual summary statistics
```


### Comparison of Estimation Methods

1. **Root Mean Square Error (RMSE)**:
   - The RMSE represents the Euclidean distance between estimated and true coefficients
   - For multiplier = 1: Both methods achieve similar RMSE (≈ 0.04)
   - For multiplier = 8: Custom likelihood shows 5-15 times smaller RMSE than coxph
   - Note: RMSE interpretation depends on coefficient scale; for multiplier = 8, same absolute error represents smaller relative error

2. **Maximum Relative Error**:
   - This metric identifies the worst-performing coefficient estimate
   - For multiplier = 8, the custom likelihood approach shows 3-10 times smaller maximum relative error compared to coxph
   - This suggests better stability in the most challenging estimation scenarios

### Understanding the Performance Difference

The superior performance of the custom likelihood optimizer can be attributed to its handling of extreme values:

1. **Standard coxph Implementation**:
   - Large coefficients → Large β'X terms
   - Results in extreme values in the hazard function
   - Leads to numerical instability in gradient calculations

2. **Custom Likelihood Approach**:
   - Identifies entries that would lead to extreme risk sums
   - Excludes these terms from the likelihood calculation
   - Effectively prevents gradient explosion
   - Maintains stability even with large coefficients



## Visualization
```{r}
create_comparison_plots <- function(comparison_table) {
  # Reshape data for plotting
  plot_data_rmse <- data.frame(
    Seed = comparison_table$Seed,
    Multiplier = as.numeric(comparison_table$Multiplier),
    Custom = comparison_table$RMSE_custom_w,
    Coxph = comparison_table$RMSE_coxph_w
  ) %>%
    tidyr::pivot_longer(cols = c(Custom, Coxph), 
                       names_to = "Method", 
                       values_to = "RMSE")

  plot_data_maxrel <- data.frame(
    Seed = comparison_table$Seed,
    Multiplier = as.numeric(comparison_table$Multiplier),
    Custom = comparison_table$max_rel_err_custom_w,
    Coxph = comparison_table$max_rel_err_coxph_w
  ) %>%
    tidyr::pivot_longer(cols = c(Custom, Coxph), 
                       names_to = "Method", 
                       values_to = "MaxRelError")

  plot_data_loglik <- data.frame(
    Seed = comparison_table$Seed,
    Multiplier = as.numeric(comparison_table$Multiplier),
    Custom = comparison_table$LogLik_custom_w,
    Coxph = comparison_table$LogLik_coxph_w
  ) %>%
    tidyr::pivot_longer(cols = c(Custom, Coxph), 
                       names_to = "Method", 
                       values_to = "LogLik")

  # Create individual plots
  p1 <- ggplot(plot_data_rmse, 
               aes(x = factor(Seed), y = RMSE, fill = Method)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~Multiplier, scales = "free_y", 
               labeller = labeller(Multiplier = function(x) paste("Multiplier:", x))) +
    labs(title = "RMSE Comparison",
         x = "Seed",
         y = "RMSE") +
    theme_minimal() +
    theme(legend.position = "none")

  p2 <- ggplot(plot_data_maxrel, 
               aes(x = factor(Seed), y = MaxRelError, fill = Method)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~Multiplier, scales = "free_y",
               labeller = labeller(Multiplier = function(x) paste("Multiplier:", x))) +
    labs(title = "Maximum Relative Error Comparison",
         x = "Seed",
         y = "Max Relative Error") +
    theme_minimal() +
    theme(legend.position = "none")

  p3 <- ggplot(plot_data_loglik, 
               aes(x = factor(Seed), y = LogLik, fill = Method)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~Multiplier, scales = "free_y",
               labeller = labeller(Multiplier = function(x) paste("Multiplier:", x))) +
    labs(title = "Log-Likelihood Comparison",
         x = "Seed",
         y = "Log-Likelihood") +
    theme_minimal() +
    theme(legend.position = "bottom")

  # Combine plots
  combined_plot <- cowplot::plot_grid(
    p1, p2, p3,
    nrow = 3,
    align = "v",
    axis = "lr",
    rel_heights = c(1, 1, 1.2)  # Make bottom plot slightly taller to accommodate legend
  )

  return(combined_plot)
}

# Create and display the combined plot
combined_plots <- create_comparison_plots(comparison_table)
combined_plots
```




## Conclusions

1. **Coefficient Magnitude Matters**:
   - Small coefficients (multiplier = 1): Both methods perform well
   - Large coefficients (multiplier = 8): Standard coxph shows significant instability

2. **Custom Likelihood Advantage**:
   - More robust to large coefficients
   - Maintains estimation accuracy through careful handling of extreme values
   - Provides a practical solution for challenging survival analysis scenarios

3. **Practical Implications**:
   - Standard implementations may need careful monitoring when coefficients are large
   - Custom likelihood approaches offer a more robust alternative
   - Important consideration for survival analysis with extreme hazard values

These findings suggest that practitioners should be cautious when using standard Cox model implementations with large coefficients and consider robust alternatives when dealing with extreme values in the hazard function.


## TODO
1. Compare results and show offsets does not matter.
2. Get clearer why ignoring large risk_sum could work. One way is to see when encountering large risk_sum, how many data it ignores and what is the next beta it run to. 
